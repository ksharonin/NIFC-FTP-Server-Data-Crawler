{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d132b9c",
   "metadata": {},
   "source": [
    "\n",
    "# *NASA-NIFC Incident Specific Data Web Crawler V1*\n",
    "### By Katrina Sharonin\n",
    "\n",
    "## Description:\n",
    "The Incident Data Web Crawler is a recursive prototype program which given a user input (keyword) and starting FTP database URL, will traverse the entire tree of file directories to locate URLS containing the keyword. This program aims to accelerate the search of incident fire data given a fire name.\n",
    "\n",
    "The NIFC FTP Server (https://ftp.wildfire.gov/) is the an official site for interagency wildland fire incident data and documents. Valuable data for several incidents is store on the server, with data such as historic fire perimeters, IR mission guides, IR category maps, incident management documents, etc. The server claims:\n",
    "\n",
    ">'This ftp service is intended for short-term interagency sharing, not as a file archive or records repository. There shouldn't be anything data that isn't stored in a safer location, or much data that carries over from season to season.' \n",
    "\n",
    "However, most data for infrared operations and perimeters, mainly CAL FIRE, is stored here in a long-term manner. Making a public request for data from CAL FIRE will put most users here.\n",
    "\n",
    "Upon a simple inspection (visit https://ftp.wildfire.gov/public/incident_specific_data/ to see the incident specific data portion of the data), the structure of the FTP database demonstrates a flawed structure: <span style='color:Red'> **no search option is available. Users must manually search through the database by clicking through directories.** </span>\n",
    "\n",
    "Given the slow and inefficent structure, this script accelerates search for quick location of research-relevant data.\n",
    "\n",
    "## Inspiration:\n",
    "Comparing NASA Tools means comparing NASA data on specific incidents. For example, the MASTER tool has its own archive of fire campaigns, with names included (https://daac.ornl.gov/cgi-bin/dataset_lister.pl?p=43). Research centered on comparing NASA data VS. wildfire management collected data requires locating the data in the FTP directories. But...\n",
    "\n",
    "<span style='color:Maroon'>\n",
    "    \n",
    "#### **My personal experience was painful to say the least: I spent hours looking for certain fires only to find they don't exist, or got put into some federal and not state folder. The \"organization\" of data interferes with its usability.**\n",
    "    \n",
    "#### **The interference of the data's acessibility ultimately impacts research. The fact this script needs to exist demonstrates a fault in current data storage for wildfire incidents.**\n",
    "    \n",
    "## **If we want good research, we need usable, relevant, and accessible quality data**\n",
    "\n",
    "#### **By automating search we can understand how much incident specific data is truly present and how much is not, how many fires exist that line-up with NASA incident data, and so on. All in all the search will help characterize available data in order to provide points of weakness, rather than saying \"we need more data\"**\n",
    "</span>\n",
    "\n",
    "## Goals:\n",
    "- Take user input of fire\n",
    "- Search given starting URL directories \n",
    "- When matching name is found, **return link(s) to directory with the name contained (provide print statements)**\n",
    "- Future development add ons: quantity of net files (rather than just dirs existing) -> calculate percent of empty dirs\n",
    "\n",
    "***\n",
    "\n",
    "## Part 1: Welcome + Process User Input\n",
    "- Run block to accept user input\n",
    "- Take input and process for use in the incoming blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97087634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Welcome to the Incident Data Web Crawler ----\n",
      "\n",
      "To get started, please run the blocks below in sequential order using Shift + Enter\n",
      "\n",
      "This program will take your keyword and start from the the Incident Specific Data/ directory.\n",
      "The final block will print out results, it may take a few minutes due to the sheer size.\n",
      "\n",
      "Sample inputs: \"Fort Huachuca\", \"Avenza\", \"Melozitna\", \"Dixie\", \"Contact Creek\"\n",
      "\n",
      "Report any bugs to katrina.sharonin@nasa.gov\n"
     ]
    }
   ],
   "source": [
    "# RUN ME FOR USER WELCOME - Press Shift+Enter\n",
    "\n",
    "# import libraries to parse and read through the websites \n",
    "import bs4, requests\n",
    "import plyer\n",
    "import time \n",
    "\n",
    "print('---- Welcome to the Incident Data Web Crawler ----')\n",
    "print('')\n",
    "print('To get started, please run the blocks below in sequential order using Shift + Enter')\n",
    "print('')\n",
    "print('This program will take your keyword and start from the the Incident Specific Data/ directory.')\n",
    "print('The final block will print out results, it may take a few minutes due to the sheer size.')\n",
    "print('')\n",
    "print('Sample inputs: \"Fort Huachuca\", \"Avenza\", \"Melozitna\", \"Dixie\", \"Contact Creek\"')\n",
    "print('')\n",
    "print('Report any bugs to katrina.sharonin@nasa.gov')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f85b377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search keyword.\n",
      "NOTE: Enter incident name in ALL lower case with no \"Fire\" or \"fire\" term. Please avoid underscores. Include spaces in between for names i.e. Creek Forest, not CreekForest for optimal results\n",
      " \n",
      "Please input the incident name you are searching for: harrison gulch\n",
      "\n",
      "Name \"harrison gulch\" recieved.\n",
      "The keyword contained a space, it has been replaced with a \"%20\" to match URL encoding\n",
      " \n",
      "Final keyword for input:\n",
      "'harrison%20gulch'\n",
      "\n",
      "Successful Input. Initiating search...\n"
     ]
    }
   ],
   "source": [
    "# RUN ME TO ACCEPT USER INPUT FOR KEYWORD\n",
    "\n",
    "# Accept user prompt: Run this block to gather user input on the fire wanted\n",
    "# after acceptance, regex will be applied to the key term -> grab capital alikes \n",
    "# 4+ situations:\n",
    "# 1. King (first letter capitalized)\n",
    "# 2. KING (all caps)\n",
    "# 3. king (all lower)\n",
    "# 4. King Fire (etc versions of these) -> exclude anything aligning with fire\n",
    "\n",
    "print('Enter your search keyword.')\n",
    "print('NOTE: Enter incident name in ALL lower case with no \"Fire\" or \"fire\" term. Please avoid underscores. Include spaces in between for names i.e. Creek Forest, not CreekForest for optimal results')\n",
    "print(' ')\n",
    "keyword = input('Please input the incident name you are searching for: ')\n",
    "print('')\n",
    "print('Name \"' + keyword  + '\" recieved.')\n",
    "\n",
    "# in attempt to normalize all inputs, put into lower case \n",
    "keyword = keyword.lower()\n",
    "\n",
    "# check if the word 'fire' || 'Fire' exists. if true, remove\n",
    "if 'fire' in keyword:\n",
    "    print('The keyword \"fire\" was found in your input. Normalizing...')\n",
    "    keyword = keyword.replace(\"fire\", \"\")\n",
    "if 'Fire' in keyword:\n",
    "    # in case lower fails -> catch later!\n",
    "    print('The keyword \"Fire\" was found in your input. Normalizing...')\n",
    "    keyword = keyword.replace(\"Fire\", \"\")\n",
    "    \n",
    "# remove any spaces within the string for easy search - do not string in-between values \n",
    "keyword = keyword.strip()\n",
    "\n",
    "# match encoding of inside spaces - \" \" do not exist in URLs of FTP server!\n",
    "# also remove _ as they are mixed with spaces -> this way processing program treats them fairly\n",
    "if \" \" in keyword:\n",
    "    keyword = keyword.replace(\" \", \"%20\")\n",
    "    print('The keyword contained a space, it has been replaced with a \"%20\" to match URL encoding')\n",
    "if \"_\" in keyword:\n",
    "    keyword = keyword.replace(\"_\", \"%20\")\n",
    "    print('The keyword contained an underscore, it has been replaced with a \"%20\" to match URL encoding')\n",
    "\n",
    "print(' ')\n",
    "print('Final keyword for input:')\n",
    "print(\"'\"+ keyword + \"'\")\n",
    "\n",
    "# Check for any capitals or spaces\n",
    "try:\n",
    "        assert keyword.islower(), \"Passed input is not lower-case, halt program. Please input your incident as lower case with no key term of 'Fire' or 'fire' included\"\n",
    "except AssertionError as msg:\n",
    "        print(msg)\n",
    "\n",
    "print('')\n",
    "print('Successful Input. Initiating search...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dfb0ed",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Part 2: Main functions\n",
    "\n",
    "***\n",
    "\n",
    "## Design considerations and notes during dev:\n",
    "- Currently the ftp gov URL is organized by appending directory names -> per searching just recursively grab names until its either finished looping through ALL directories for a name\n",
    "- if the name is not lower, need to lower case all letters in the directory to match with lower() method used on the keyword\n",
    "- append to all possible present directories that have name -> keep searching till directory is \n",
    "\n",
    "- Ultimately return the directories, not the files (so should not have . at the end, instead should end with slash!)\n",
    "- EX: /public/incident_specific_data/calif_n/2020_FEDERAL_Incidents/CA-KNF-007035_Slater/IR/NIROPS -> the parent is excluding '/NIROPS'\n",
    "- look for title 'Parent directory' -> exclude from the aref class loop list\n",
    "- GOAL: Capture all directories with the names present -> may include many subdirs captured\n",
    "\n",
    "Ex dir and wanted results:\n",
    "- main\n",
    "   --> NIROPS\n",
    "       --> Training\n",
    "       --> ttfs\n",
    "           --> phoneix_samples\n",
    "               --> getty_fire_shape\n",
    "   --> calin\n",
    "       --> Getty_fire (this directory name shouldve been detected)\n",
    "           --> getty.shp (these files should be detected by the program)\n",
    "           --> getty.zip\n",
    "       --> South fire\n",
    "   --> calis\n",
    "       --> Tahoe fire\n",
    "       --> sad fire\n",
    "       \n",
    "If 'getty' was used, the following links to dirs should return:\n",
    "- https.../calin\n",
    "- https.../Getty_fire\n",
    "- https.../phoenix_samples\n",
    "print(The following directories contain files that match your search!)\n",
    "\n",
    "**Consequently, this may cause dir subduplicates to get returned. Nevertheless it narrows down searches to links only where the name appears!**\n",
    "\n",
    "***\n",
    "\n",
    "## Function 1: searchForKeyword()\n",
    "\n",
    "- Description: given a page URL, return true if the keyWord is found within the listed files/directories listed in the URL. I.e. If in https://ftp.wildfire.gov/public/incident_specific_data/ , then if an input of 'fuels' would be given, it would return true as there is a folder titled 'Fuels/' in this link\n",
    "- Inputs: pageURL (string), keyWord (string)\n",
    "- Output: boolean\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e9e3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TO INITIATE SEARCHFORKEYWORD() FUNCTION IN MEMORY\n",
    "\n",
    "# Repeat imports incase restart of kernel wipes local var memory\n",
    "import bs4, requests\n",
    "import plyer\n",
    "import time \n",
    "\n",
    "# searchForKeyword() function\n",
    "# Description: given a page URL, return true if the keyWord is found within the listed files/directories listed in the URL.\n",
    "# Inputs: pageURL (string), keyWord (string)\n",
    "# Output: boolean\n",
    "\n",
    "def searchForKeyword(pageURL, keyWord):\n",
    "\n",
    "    # Download page\n",
    "    # Example of valid pageURL string input: 'https://ftp.wildfire.gov/public/incident_specific_data/'\n",
    "    \n",
    "    # try status and throw if forbidden is encountered\n",
    "    try:\n",
    "        getPage = requests.get(pageURL)\n",
    "        getPage.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as err: \n",
    "        # in case of forbidden access URL \n",
    "        print('Accessed a Forbidden URL/URL with error status, return false')\n",
    "        return False\n",
    "    except requests.exceptions.Timeout:\n",
    "        # Maybe set up for a retry, or continue in a retry loop\n",
    "        print('Timeout occured, check FTP site status manually')\n",
    "        return False\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        # Tell the user their URL was bad and try a different one\n",
    "        print('Too many re-directs, check FTP site status manually')\n",
    "        return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # catastrophic error. bail.\n",
    "        print('Catostrophic error, bail execution due to RequestException')\n",
    "        return False\n",
    "  \n",
    "    # Parse text for opportunities block\n",
    "    soup = bs4.BeautifulSoup(getPage.text, 'html.parser')\n",
    "    \n",
    "    # check the current text for any emptiness \n",
    "    a_categories = soup.find_all('a')\n",
    "    # pop out using loop to exclude irrelevant categories \n",
    "    a_categories_modified = a_categories.copy()\n",
    "    \n",
    "    if not a_categories:\n",
    "        print('Empty page detected, return false')\n",
    "        return False\n",
    "    \n",
    "    # loop through the a_categories to get rid of irreleveant matches like parent dir\n",
    "    # Also eliminate the Name, Last modified, size from the list to reduce search confusion\n",
    "        \n",
    "    # before filtering - debugging\n",
    "    # print(a_categories)\n",
    "    \n",
    "    for link in a_categories:\n",
    "        \n",
    "        # eliminate from the link search to prevent recursive returns:\n",
    "        # parent directory, name, last modified, size, description\n",
    "        \n",
    "        # Note: the filter assumes uniformity per every page for its a-class titles\n",
    "\n",
    "        # want to eliminate the possibility of lower case misses -> make whole thing lower\n",
    "        if 'Parent Directory' in str(link):\n",
    "            # remove this link from the a_categories_modified\n",
    "            a_categories_modified.remove(link)\n",
    "        elif 'Name' in str(link) or  'Size' in str(link) or 'Last modified' in str(link) or 'Description' in str(link):\n",
    "            # remove related categories that do not contribute to search, aka redundant categories\n",
    "            a_categories_modified.remove(link)\n",
    "    \n",
    "    \n",
    "    # after filtering - debugging\n",
    "    # print(a_categories_modified)\n",
    "    \n",
    "    # define default boolean which will be switched on if found\n",
    "    available = False\n",
    "\n",
    "    \n",
    "    # search the modified list of valid links, including files\n",
    "    for link in a_categories_modified:\n",
    "        # fetch the a class name of the link\n",
    "        current_string_check = link.get('href')\n",
    "        \n",
    "        # print search - debugging\n",
    "        # print(str(current_string_check).lower())\n",
    "        \n",
    "        if keyWord in str(current_string_check).lower():\n",
    "            available = True\n",
    "            break\n",
    "        # try: if there is %20 -> check one side and the other for appearance\n",
    "        # check is \"_\" can appear instead of spaces\n",
    "        elif (\"%20\" in keyWord):\n",
    "            # ISSUE: may be more than one %20 -> iterate through list \n",
    "            split = keyWord.split(\"%20\")\n",
    "            # excluding the %20, try both sides\n",
    "            # ex: Minto_Lakes != Minto%20Lakes\n",
    "            # if %20 isn't in, it may bug bc it wants whole thing\n",
    "            \n",
    "            # [true, true] -> counter should = 2\n",
    "            counter_found = 0;\n",
    "            \n",
    "            # iterate through splits to see if all are in\n",
    "            for substring in split:\n",
    "                if substring in str(current_string_check).lower():\n",
    "                    counter_found += 1\n",
    "                \n",
    "            # if the counter matches arr size -> must be true for all substrings\n",
    "            # therefore the keyword is found\n",
    "            if len(split) == counter_found:\n",
    "                available = True\n",
    "                break\n",
    "                \n",
    "\n",
    "    # print('Process finished for:' + pageURL)\n",
    "    # print('For input word in the given dir, \"' + keyWord + '\", its existence is')\n",
    "    # print('Entered keyword function')\n",
    "            \n",
    "    # If key offering found, return true\n",
    "    if available == True:\n",
    "        return True\n",
    "\n",
    "    # Otherwise, return false \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "# TEST CALLS - Debugging Only:\n",
    "# searchForKeyword('https://ftp.wildfire.gov/public/incident_specific_data/', 'getty')\n",
    "# searchForKeyword('https://ftp.wildfire.gov/public/incident_specific_data/.swp', 'getty')\n",
    "# searchForKeyword('https://ftp.wildfire.gov/public/incident_specific_data/calif_n/!CALFIRE/2013_Incidents/CA-BTU-005638-Cedar/GIS/', 'getty')\n",
    "# searchForKeyword('https://ftp.wildfire.gov/public/incident_specific_data/alaska/2022/', 'minto%20lakes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade11d37",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Function 2: crawler()\n",
    "- Description: given a starting URL (defaults to 'https://ftp.wildfire.gov/public/incident_specific_data/', keyword (user-input defined), crawler() will go through each directory in depth first search (DFS). If the URL returns true for searchForKeyword(), then the URL is printed with a success message. Otherwise, the program will note that it continues recursing. There is no return value.\n",
    "- Inputs: keyword (string which was inputted by user and processed in earlier block), startingPageURL (string)\n",
    "- Output: NONE (side-effects of printing)\n",
    "\n",
    "\n",
    "- **Note: future versions of script may attempt to form an array from found URLs. However due to recursive nature, the array can get corrupted or overflowed with information**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90d1cdad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/BaseData/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/BaseData/Arizona/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/BaseData/ForestData/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/BaseData/ForestData/R06_CSA4/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/BaseData/ForestData/R06_UMF/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/BaseData/ForestData/R06_UMF/Ellis/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/BaseData/UpdateNationalBaseMap/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZCNF/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZCNF/Cochise%20Stronghold%20Project/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZCNF/Fort%20Huachuca%20RX/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZCNF/RX%20310/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZCNF/Sierra%20Vista%20RX/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2019/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2020/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2020/AOR_Maps/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2020/AOR_Maps/QRcodes/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2020/Aviation/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2020/Aviation/2020/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2020/Closure_Signs/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2020/Closure_Signs/OpsMaps/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2020/Dispatch/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2020/Forest_Maps/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2020/LEO/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2021/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2021/Aviation/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2021/Fires/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2021/Fires/2021_Slate/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2021/Fuels/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2021/Fuels/FRD/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2021/Fuels/MRRD/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2021/ZonedAviation/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2022/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_COF/Products/2022/MRRD/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/AZ_GCP/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_ANF/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_ANF/Div1/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_ANF/Div2/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_ANF/Div3/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_BDF/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_BDF/Shapefiles/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/DRD/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/DRD/2019/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/DRD/2020/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/DRD/2021/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/DRD/2022/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/PRD/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/PRD/2018/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/PRD/2018/Admin/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/PRD/2018/BirchHill/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/PRD/2018/BirchHill/Product/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/PRD/2018/PMHLP/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_CNF/PRD/2018/PMHLP/Map%20Product/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_North%20Zone%20Monitoring%20Resources/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_North%20Zone%20Monitoring%20Resources/Fire%20Effects%20Monitoring%20(FEMO)/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_North%20Zone%20Monitoring%20Resources/Fire%20Effects%20Monitoring%20(FEMO)/Example%20FEMO%20Reports/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_North%20Zone%20Monitoring%20Resources/Fuel%20Moisture%20Monitoring/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_North%20Zone%20Monitoring%20Resources/Vegetation%20and%20Fuels%20Monitoring/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_North%20Zone%20Monitoring%20Resources/Vegetation%20and%20Fuels%20Monitoring/Field%20Forms%20as%20Excel/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_North%20Zone%20Monitoring%20Resources/Vegetation%20and%20Fuels%20Monitoring/Field%20Forms%20as%20PDF/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_SHF/\n",
      "\n",
      "NAME FOUND! Printing directory URL that has \"harrison%20gulch\" in it...\n",
      "https://ftp.wildfire.gov/public/incident_specific_data/Fuels/CA_SHF/\n",
      "\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/GMFL0920/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/GMFL0920/Hector%20Ranger%20District%20RX%20Fire%20Maps/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/GMFL0920/Manchester%20District%20RX%20Fire%20Maps/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/GMFL0920/Rochester%20Middlebury%20District%20RX%20Fire%20Maps/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/ID-BOF/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/ID-BOF/D5/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/ID-BOF/D5/Lowman%20WUI/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/ID-BOF/D5/Lowman%20WUI/Thinning%20Maps/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/ID-BOF/D5/West%20Lowman/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/ID-BOF/D5/West%20Lowman/Project%20Area%20Maps/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/ID-BOF/D5/West%20Lowman/Thinning%20Maps/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/ID-BOF/D5/West%20Lowman/Unit%20Maps/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/ID-BOF/D5/West%20Lowman/Unit%20Maps/ClearCreek/\n",
      "Recursing in: https://ftp.wildfire.gov/public/incident_specific_data/Fuels/ID-BOF/D5/West%20Lowman/Unit%20Maps/LickCreek/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-8eaa8424c6b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# directly have the initial array modified and then returned after recursive calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mfound_URL_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarting_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# print(found_URL_matches)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-8eaa8424c6b4>\u001b[0m in \u001b[0;36mcrawler\u001b[0;34m(currPageURL, keyword)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# recursive call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mcrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_URL_to_recurse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# return found_URL_matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-8eaa8424c6b4>\u001b[0m in \u001b[0;36mcrawler\u001b[0;34m(currPageURL, keyword)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# recursive call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mcrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_URL_to_recurse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# return found_URL_matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-8eaa8424c6b4>\u001b[0m in \u001b[0;36mcrawler\u001b[0;34m(currPageURL, keyword)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# recursive call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mcrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_URL_to_recurse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# return found_URL_matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-8eaa8424c6b4>\u001b[0m in \u001b[0;36mcrawler\u001b[0;34m(currPageURL, keyword)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# recursive call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mcrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_URL_to_recurse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# return found_URL_matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-8eaa8424c6b4>\u001b[0m in \u001b[0;36mcrawler\u001b[0;34m(currPageURL, keyword)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# recursive call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mcrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_URL_to_recurse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# return found_URL_matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-8eaa8424c6b4>\u001b[0m in \u001b[0;36mcrawler\u001b[0;34m(currPageURL, keyword)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# recursive call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mcrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_URL_to_recurse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# return found_URL_matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-8eaa8424c6b4>\u001b[0m in \u001b[0;36mcrawler\u001b[0;34m(currPageURL, keyword)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# search the current page with boolean returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mkeyword_boolean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearchForKeyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrPageURL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeyword_boolean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-411f9e637aac>\u001b[0m in \u001b[0;36msearchForKeyword\u001b[0;34m(pageURL, keyWord)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# try status and throw if forbidden is encountered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mgetPage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpageURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mgetPage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_default_certs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[1;32m    412\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0mkeyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0m\u001b[1;32m    429\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# ctx._wrap_socket()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return self.sslsocket_class._create(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1038\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RUN TO BEING SEARCH USING CRAWLER() FUNCTION\n",
    "\n",
    "# Repeat imports incase restart of kernel wipes local var memory\n",
    "import bs4, requests\n",
    "import plyer\n",
    "import time \n",
    "\n",
    "# crawler() function\n",
    "# Description: \n",
    "# Inputs: keyword (string which was inputted by user and processed in earlier block), startingPageURL (string)\n",
    "# Output: found_URL_matches (array with all URL strings found with matches)\n",
    "\n",
    "# starting URL -> this can be customized, but is used to reduce the search span for the function \n",
    "# DEFAULT:\n",
    "\n",
    "starting_URL = 'https://ftp.wildfire.gov/public/incident_specific_data/'\n",
    "# empty array which will be returned \n",
    "# append to array after every recursive call to the searchForKeyword function \n",
    "# found_URL_matches = []\n",
    "\n",
    "def crawler(currPageURL, keyword):\n",
    "    \n",
    "    # search the current page with boolean returned\n",
    "    keyword_boolean = searchForKeyword(currPageURL, keyword)\n",
    "    \n",
    "    if keyword_boolean:\n",
    "        # if found, display result\n",
    "        print('')\n",
    "        print('NAME FOUND! Printing directory URL that has \"' + keyword + '\" in it...')\n",
    "        print(currPageURL)\n",
    "        print('')\n",
    "        # append the URL to arr\n",
    "        # found_URL_matches.append(currPageURL)\n",
    "        \n",
    "        # TEMPORARY\n",
    "        # return\n",
    "        \n",
    "    # now for every directory, append -> create URL -> recurse\n",
    "    # make sure to update the stored ver \n",
    "    # this could cause issues depending on how it traverses the file tree\n",
    "    \n",
    "    # repeat get page process for the current URL before access\n",
    "    try:\n",
    "        getPage = requests.get(currPageURL)\n",
    "        getPage.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as err: \n",
    "        # in case of forbidden access URL \n",
    "        print('Accessed a Forbidden URL/URL with error status, return false')\n",
    "        return False\n",
    "    except requests.exceptions.Timeout:\n",
    "        # Maybe set up for a retry, or continue in a retry loop\n",
    "        print('Timeout occured, check FTP site status manually')\n",
    "        return False\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        # Tell the user their URL was bad and try a different one\n",
    "        print('Too many re-directs, check FTP site status manually')\n",
    "        return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # catastrophic error. bail.\n",
    "        print('Catostrophic error, bail execution due to RequestException')\n",
    "        return False\n",
    "  \n",
    "    # Parse text for opportunities block\n",
    "    soup = bs4.BeautifulSoup(getPage.text, 'html.parser')\n",
    "    \n",
    "    # check the current text for any emptiness \n",
    "    a_categories = soup.find_all('a')\n",
    "    # pop out using loop to exclude irrelevant categories \n",
    "    a_categories_modified = a_categories.copy()\n",
    "    \n",
    "    # loop through the a_categories to get rid of irreleveant matches like parent dir\n",
    "    # Also eliminate the Name, Last modified, size from the list to reduce search confusion\n",
    "        \n",
    "    for link in a_categories:\n",
    "        # eliminate from the link search to prevent recursive returns:\n",
    "        # want to eliminate the possibility of lower case misses -> make whole thing lower\n",
    "        if 'Parent Directory' in str(link):\n",
    "            # remove this link from the a_categories_modified\n",
    "            a_categories_modified.remove(link)\n",
    "        elif 'Name' in str(link) or  'Size' in str(link) or 'Last modified' in str(link) or 'Description' in str(link):\n",
    "            # remove related categories that do not contribute to search, aka redundant categories\n",
    "            a_categories_modified.remove(link)\n",
    "    \n",
    "    \n",
    "    for link in a_categories_modified:\n",
    "        # recurse and pick directories\n",
    "        # if is a directory -> call function with the URL name appended to meet it\n",
    "        actual_URL = str(link.get('href'))\n",
    "        # if ends with '/' -> is a directory -> recurse \n",
    "        # DO NOT RECURSE ON FILES!\n",
    "        if actual_URL.endswith('/'):\n",
    "            #p rint(actual_URL)\n",
    "            # print('Directory detected through / in link')\n",
    "            \n",
    "            # modify currentURL by appending this\n",
    "            new_URL_to_recurse = currPageURL + actual_URL\n",
    "            \n",
    "            # recurse on this file branch\n",
    "            print('Recursing in: ' + new_URL_to_recurse)\n",
    "            # print(found_URL_matches)\n",
    "            # found_URL_matches.append(crawler(new_URL_to_recurse, keyword, []))\n",
    "            \n",
    "            # recursive call\n",
    "            crawler(new_URL_to_recurse, keyword)\n",
    "    \n",
    "    # return found_URL_matches\n",
    "    return 1\n",
    "\n",
    "\n",
    "# directly have the initial array modified and then returned after recursive calls\n",
    "found_URL_matches = crawler(starting_URL, keyword)\n",
    "\n",
    "# print(found_URL_matches)\n",
    "\n",
    "# BUG: Fort%20Huachuca%20RX != match with 'fort huachuca'\n",
    "# likeley due to mandated space -> then needs to be set of keywords or replace space with %20?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bf7d1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "fort%20huachuca\n"
     ]
    }
   ],
   "source": [
    "# test space here for string equivelencies\n",
    "\n",
    "#Fort%20Huachuca%20RX != match with 'fort huachuca'\n",
    "\n",
    "lower = \"Fort%20Huachuca%20RX\"\n",
    "lower = lower.lower()\n",
    "keyWord = 'fort huachuca'\n",
    "\n",
    "boolean = keyWord in lower\n",
    "print(boolean)\n",
    "\n",
    "keyWord = keyWord.replace(\" \", \"%20\")\n",
    "print(keyWord)\n",
    "\n",
    "# therefore need to replace spaces with %20 mark!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb96a59",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
